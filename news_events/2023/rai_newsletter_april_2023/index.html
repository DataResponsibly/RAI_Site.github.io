<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>R/AI Newsletter - Research Highlights Issue, April 2023 | Welcome to the Center for Responsible AI at New York University</title> <meta name="author" content="Center for Responsible AI at New York University"> <meta name="description" content="Center for Responsible AI at New York University. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://airesponsibly.net/news_events/2023/rai_newsletter_april_2023/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "R/AI Newsletter - Research Highlights Issue, April 2023",
      "description": "",
      "published": "April 21, 2023",
      "authors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="header-image"></div> <div class="container"> <div style="height: 100%"><a href="/"><img src="https://i.postimg.cc/QMJ7NxZ9/rai.png" alt="logo"></a></div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">News &amp; Events</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">People</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/people/#team">Team</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/people/#visitors">Visitors</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/research/">Overview</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#data-centric">Data-centric AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#explainability">Explainability</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#fairness">Fairness</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#policy">Policy</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#privacy">Privacy</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#ranking">Ranking</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/research/#education">RAI education</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Education</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/education/">Overview</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#rds">Responsible data science</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#playground">Causal inference</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#playbook">Algorithmic transparency</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#algorithmia">Algorithmia</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#weareai">We are AI</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/education/#comics">Comics</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Policy</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/policy/">Overview</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/policy/#governance">AI governance</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/policy/#hiring">Algorithmic hiring</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">RAIforUkraine</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/RAIforUkraine/">Overview</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/RAIforUkraine/#students">Students</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/RAIforUkraine/#mentors">Mentors</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/RAIforUkraine/#donors">Donors</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/RAIforUkraine/#publications">Publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/RAIforUkraine/#press">Press</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>R/AI Newsletter - Research Highlights Issue, April 2023</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introducing-contextual-transparency-for-automated-decision-systems">Introducing Contextual Transparency for Automated Decision Systems</h2> <div class="row mt-3"> <div class="col-sm mt-10 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/Contextual.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>LinkedIn Recruiter — a search tool used by professional job recruiters to find candidates for open positions — would function better if recruiters knew exactly how LinkedIn generates its search query responses. This is what the concept of “contextual transparency” focuses on. In our new paper “Contextual transparency for automated decision systems” (Nature Machine Intelligence, March 2023), NYU R/AI’s Mona Sloane and Julia Stoyanovich collaborated with R/AI alum Ian René Solano-Kamaiko, and Jun Yuan and Aritra Dasgupta from NJIT, on a new method for combining social science, engineering, and design to automate the creation of “nutritional labels” for Automated Decision System (ADS). The label lays bare the explicit and hidden criteria — the ingredients and the recipe — within the algorithms or other technological processes the ADS uses in specific situations, such as candidate sourcing. Contextual transparency can be used to respond to AI transparency requirements that are mandated in new and forthcoming AI regulation in the US and Europe, such as the NYC Local Law 144 of 2021 or the EU AI Act.</p> <p><a href="https://engineering.nyu.edu/news/better-transparency-introducing-contextual-transparency-automated-decision-systems" rel="external nofollow noopener" target="_blank"><strong>Read more here</strong></a>, find the Nature Machine Intelligence paper <a href="https://www.nature.com/articles/s42256-023-00623-7.epdf?sharing_token=mUWY2nNDVRkXAk3rPaRLUdRgN0jAjWel9jnR3ZoTv0P9184ur6aW-z48bzYkoPmlsTYPtIvPCAk_ayHnwLBpBawLWNn2-mpn_OaFiud6O17PfVT6wy35-4YKSFMiVpkylJFsS_-0OR3zaq1cfg4Cn9BmrMJA3qZLUjiPiAQwjbM%3D" rel="external nofollow noopener" target="_blank"><strong>here</strong></a>.</p> <h2 id="what-were-looking-forward-to">What we’re looking forward to:</h2> <h3 id="the-algorithmic-transparency-playbook-a-stakeholder-first-approach-to-creating-transparency-in-your-organizations-algorithmsthe-algorithmic-transparency-playbook-a-stakeholder-first-approach-to-creating-transparency-in-your-organizations-algorithms"><a href="https://programs.sigchi.org/chi/2023" rel="external nofollow noopener" target="_blank">The Algorithmic Transparency Playbook: A Stakeholder-first Approach to Creating Transparency in Your Organization’s AlgorithmsThe Algorithmic Transparency Playbook: A Stakeholder-first Approach to Creating Transparency in Your Organization’s Algorithms</a></h3> <p>Andrew Bell, Oded Nov, Julia Stoyanovich Course at the 2023 Computer-Human Interaction Conference (<a href="https://chi2023.acm.org/" rel="external nofollow noopener" target="_blank"><strong>CHI’23</strong></a>)</p> <div class="row mt-3"> <div class="col-sm mt-10 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/Algo-transparency.webp" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Do you believe in <strong>transparency</strong>, <strong>accountability</strong>, and <strong>openness</strong> for algorithmic systems, like those you hear about in the news, a la ChatGPT?</p> <p>The NYU Center for Responsible AI will be holding a critically important course at the CHI 2023 conference titled “The Algorithmic Transparency Playbook: A Stakeholder-first Approach for Creating Transparency for Your Organization’s Algorithms!” Those who participate will learn everything they need to know about algorithmic transparency, and how one can help influence change within their organization towards having more open, and accountable systems. The course also includes a case study game where participants will get to explore the tension between different key stakeholders vying for and against algorithmic transparency! This course is based on a paper, published by NYU R/AI’s Andrew Bell and <a href="https://engineering.nyu.edu/faculty/julia-stoyanovich" rel="external nofollow noopener" target="_blank"><strong>Julia Stoyanovich</strong></a>, in collaboration with <a href="https://engineering.nyu.edu/faculty/oded-nov" rel="external nofollow noopener" target="_blank"><strong>Oded Nov</strong></a> (<a href="https://www.cambridge.org/core/journals/data-and-policy/article/think-about-the-stakeholders-first-toward-an-algorithmic-transparency-playbook-for-regulatory-compliance/10D7F194DB250DDF3A30471B5CEB9326" rel="external nofollow noopener" target="_blank">Data &amp; Policy, March 2023</a>).</p> <p>If you are attending the CHI 2023 conference and are interested in registering for the course, <a href="https://web.cvent.com/event/8309e84d-5c2d-424a-9388-4a62df5ff386/" rel="external nofollow noopener" target="_blank">visit this page</a> (note that the course is labeled as Course 31).</p> <p>Check out the <a href="https://dataresponsibly.github.io/algorithmic-transparency-playbook/" rel="external nofollow noopener" target="_blank"><strong>course website</strong></a>, find the <a href="https://www.nature.com/articles/s42256-023-00623-7.epdf?sharing_token=mUWY2nNDVRkXAk3rPaRLUdRgN0jAjWel9jnR3ZoTv0P9184ur6aW-z48bzYkoPmlsTYPtIvPCAk_ayHnwLBpBawLWNn2-mpn_OaFiud6O17PfVT6wy35-4YKSFMiVpkylJFsS_-0OR3zaq1cfg4Cn9BmrMJA3qZLUjiPiAQwjbM%3D" rel="external nofollow noopener" target="_blank"><strong>playbook</strong></a>, and the <a href="https://www.cambridge.org/core/journals/data-and-policy/article/think-about-the-stakeholders-first-toward-an-algorithmic-transparency-playbook-for-regulatory-compliance/10D7F194DB250DDF3A30471B5CEB9326" rel="external nofollow noopener" target="_blank"><strong>full paper</strong></a>.</p> <h2 id="what-we-have-been-looking-forward-to">What we have been looking forward to:</h2> <h3 id="resume-format-linkedin-urls-and-other-unexpected-influences-on-ai-personality-prediction-in-hiring-results-of-an-audit"><a href="https://dl.acm.org/doi/10.1145/3514094.3534189" rel="external nofollow noopener" target="_blank">Resume Format, LinkedIn URLs and Other Unexpected Influences on AI Personality Prediction in Hiring: Results of an Audit</a></h3> <p>Alene Rhea, Kelsey Markey, Lauren D’Arinzo, Hilke Schellmann, Mona Sloane, Paul Squires, and Julia Stoyanovich. In 2022 AAAI/ACM Conference on AI, Ethics, and Society (AIES ‘22).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/Personality-prediction.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Algorithmic personality tests are in broad use in hiring today, but do they work? We sought to answer this question by interrogating the validity of algorithmic personality tests that claim to estimate a job seeker’s personality based on their resume or social media profile. We developed a methodology for auditing the stability of predictions made by these tests. Crucially, we framed our methodology around testing the assumptions made by the vendors of these tools. We used this methodology to conduct an external audit of two commercial systems, Humantic AI and Crystal, over a dataset of job applicant profiles collected through an IRB-approved study. The key take-away is that both systems show instability on key facets of measurement, and so cannot be considered valid testing instruments for pre-hire assessment.</p> <p><a href="/news_events/2022/rai-september-2022/"><strong>Read more</strong></a> about our audit, and <a href="https://www.youtube.com/watch?v=A4RSccTt3kQ&amp;t=1s" rel="external nofollow noopener" target="_blank"><strong>watch a 15-minute video</strong></a> for a summary of our methods and findings. And take a look at press coverage of these results in <a href="https://www.forbes.com/sites/drnancydoyle/2022/10/11/artificial-intelligence-is-dangerous-for-disabled-people-at-work-4-takeaways-for-developers-and-buyers/?sh=77ab36af35d3" rel="external nofollow noopener" target="_blank"><strong>Forbes</strong></a> and <a href="https://www.hrdive.com/news/as-nyc-restricts-ai-in-hiring-next-steps-remain-cloudy/633576/" rel="external nofollow noopener" target="_blank"><strong>HR Drive</strong></a>.</p> <h3 id="fairness-in-ranking-from-values-to-technical-choices-and-back"><a href="https://drive.google.com/drive/folders/1BnmGw0Z4g0f07-KTIoDlFISJ8qITEhV1" rel="external nofollow noopener" target="_blank">Fairness in Ranking: From Values to Technical Choices and Back</a></h3> <p>Julia Stoyanovich, Meike Zehlike and Ke Yang Tutorial at 2023 The Web Conference (<a href="https://www2023.thewebconf.org/" rel="external nofollow noopener" target="_blank"><strong>WWW’23</strong></a>)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/Minions-ranking.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In the past few years, there has been much work on incorporating fairness requirements into the design of algorithmic rankers, with contributions from the data management, algorithms, information retrieval, and recommender systems communities.</p> <p><a href="https://engineering.nyu.edu/faculty/julia-stoyanovich" rel="external nofollow noopener" target="_blank"><strong>Julia Stoyanovich</strong></a>, NYU R/AI alum <a href="https://www.cics.umass.edu/people/yang-ke" rel="external nofollow noopener" target="_blank"><strong>Ke Yang</strong></a>, and <a href="https://milkalichtblau.github.io/" rel="external nofollow noopener" target="_blank"><strong>Meike Zehlike</strong></a>, will give a <a href="https://www2023.thewebconf.org/program/tutorials/" rel="external nofollow noopener" target="_blank">tutorial on Fairness in Ranking</a> at The Web Conference (WWW’23). This tutorial, based on a recent two-part survey that appeared in ACM Computing Surveys (part 1, part 2) offers a broad perspective that connects formalizations and algorithmic approaches to fair ranking across subfields. During the introductory part of the tutorial, we will present a classification framework for fairness-enhancing interventions, along which we will then relate the technical methods. Next, during the main part of the tutorial, we will discuss fairness in score-based ranking and in supervised learning-to-rank. We will conclude with recommendations for practitioners, to help them select a fair ranking method based on the requirements of their specific application domain.</p> <h3 id="automated-data-cleaning-can-hurt-fairness-in-ml-based-decision-making"><a href="https://icde2023.ics.uci.edu/papers-special-track/" rel="external nofollow noopener" target="_blank">Automated Data Cleaning Can Hurt Fairness in ML-based Decision Making</a></h3> <p>Shubha Guha, Falaah Arif Khan, Julia Stoyanovich, Sebastian Schelter In Proceedings of the 39th IEEE International Conference on Data Engineering (<a href="https://icde2023.ics.uci.edu/" rel="external nofollow noopener" target="_blank"><strong>ICDE’23</strong></a>)</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/WAE.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>In this work, NYU R/AI’s Julia Stoyanovich and Falaah Arif Khan collaborated with Shubha Guha and Sebastian Schelter from the University of Amsterdam. We interrogate whether data quality issues track demographic characteristics such as sex, race and age, and whether automated data cleaning — of the kind commonly used in production ML systems — impacts the fairness of predictions made by these systems. We find that the incidence of data errors, specifically missing values, outliers and mislabels, does not (significantly) track demographic characteristics, but the downstream impact of automated cleaning is more likely to be negative from a fairness perspective (increases disparity in predictive performance) than positive. This finding is both significant and worrying, given that it potentially implicates many production ML systems, and is an indication that we need to envision new data cleaning methods — ones that are fairness-aware.</p> <p>Join the <a href="https://icde2023.ics.uci.edu/program-overview/" rel="external nofollow noopener" target="_blank">“Special Track Paper Session 2”</a> at 4pm PDT on April 6th to see the oral presentation and stay for Q/A!</p> <p><a href="https://ssc.io/pdf/demodq.pdf" rel="external nofollow noopener" target="_blank"><strong>Read the full paper</strong></a></p> <h3 id="all-aboard-making-ai-education-accessible"><a href="https://engineering.nyu.edu/events/2023/04/27/all-aboard-launch-event-collaboration-new-york-public-library" rel="external nofollow noopener" target="_blank">All Aboard! Making AI Education Accessible</a></h3> <p>Falaah Arif Khan, Lucius Bynum, Amy Hurst, Lucas Rosenblatt, Meghana Shanbhogue, Mona Sloane, Julia Stoyanovich <br> Virtual launch event in collaboration with the New York Public Library</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/p5.webp" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The All Aboard! Primer is the outcome of three roundtables that R/AI hosted between AI and social science scholars and disability experts and activists. The Primer makes concrete recommendations for making public AI education more accessible and provides:</p> <p>• Best practices and guidelines for making text-based and visual educational content accessible. • A case study that illustrates how comics can be used to accessibly communicate AI concepts to the general public. • Pointers to free resources you can use to improve the accessibility of educational content you are developing. • Join us for a discussion on how we can all make a difference in making public AI education more accessible!</p> <p><a href="https://nyu.zoom.us/webinar/register/WN_mqmySSz3TfupQiFtvC_EVw" rel="external nofollow noopener" target="_blank"><strong>Register here</strong></a></p> <h2 id="what-we-have-been-up-to">What we have been up to:</h2> <h3 id="responsible-ai-rockstar-series-moshe-vardi-visits-nyu"><a href="news_events/2023/rai_newsletter_january_2023/">Responsible AI Rockstar series: Moshe Vardi visits NYU!</a></h3> <p>Lucius E.J. Bynum, Falaah Arif Khan, Oleksandra Konopatska, Joshua R. Loftus, and Julia Stoyanovich</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/newsletters/April_2023/Moshe.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" style="width: auto; height: 400px;" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The RAI Rockstar series continued with a visit by Moshe Vardi, University Professor and George Distinguished Service Professor in Computational Engineering at Rice University. Moshe is the recipient of several awards, including the ACM SIGACT Goedel Prize, the ACM Kanellakis Award, the IEEE Computer Society Goode Award, and the EATCS Distinguished Achievements Award. He is a Senior Editor of Communications of the ACM, the premier publication in computing.</p> <p>The talk “<strong>How to be an Ethical Computer Scientist,</strong>” was held at 11am on Friday, February 10, 2023, at the NYU Center for Data Science.</p> <p><a href="https://youtu.be/2DrtSuDULEk" rel="external nofollow noopener" target="_blank">Watch the recording!</a></p> <h2 id="events-and-press-coverage">Events and Press Coverage</h2> <p>Members of R/AI spoke about our research results and recent developments in AI regulation.</p> <p><strong>Julia Stoyanovich</strong> and <a href="https://thegovlab.org/stefaan-verhulst.html" rel="external nofollow noopener" target="_blank"><strong>Stefaan Verhulst</strong></a> co-moderated the <a href="https://medium.com/data-stewards-network/diverse-approaches-to-applying-ethics-to-ai-524f855189a2" rel="external nofollow noopener" target="_blank">Global Perspectives on AI Ethics Panel #12</a>.</p> <p>“An A.I. Start-Up Boomed, but Now It Faces a Slowing Economy and New Rules,” an article in the New York Times discusses how AI is transforming the hiring process for companies. “We need to be very careful about how we use these technologies, especially in the hiring process,” said Julia.</p> <p>In an article in the Time Magazine, “<a href="https://time.com/charter/6264732/the-humane-response-to-the-robots-taking-over-our-world/" rel="external nofollow noopener" target="_blank">The Humane Response to the Robots Taking Over Our World,</a>” Julia expressed her concern about ChatGPT. She cautioned that the goal of many programs “is not to generate new text that is accurate. Or morally justifiable. It’s to sound like a human.” And so it’s entirely appropriate to treat them that way and call them out, Julia suggests, for what they sometimes are: “Bullshitters.”</p> <p>In an article Bloomberg Law, “<a href="https://news.bloomberglaw.com/daily-labor-report/workplace-ai-vendors-employers-rush-to-set-bias-auditing-bar" rel="external nofollow noopener" target="_blank">Workplace AI Vendors, Employers Rush to Set Bias Auditing Bar,</a>” Mona Sloane spoke about the lack of AI audit standards. “There is a race going on for setting the standard by way of doing it rather than waiting for a government agency to say, ‘This is what an audit of hiring AI should look like’.”</p> <p>Mona Sloane co-authored the article “The shake-up of the tech sector shows: we must learn from finance regulation.” The authors call for collaboration between the tech industry and regulators to create effective and efficient regulations.</p> <p>Julia spoke at a conference, “<a href="https://www.northjersey.com/picture-gallery/news/new-jersey/2023/03/31/njit-2023-women-designing-future-discuss-artificial-intelligence/11578144002/" rel="external nofollow noopener" target="_blank"><strong>Women Designing the Future: Artificial Intelligence/Real Human Lives</strong></a>” hosted by NJIT. <a href="https://airesponsibly.net/2023/04/22/r-ai-newsletter-research-highlights-issue-april-2023/#:~:text=In%20an%20article%20Bloomberg,read%20the%20story."><strong>Watch the full recording on YouTube</strong></a> or <a href="https://www.northjersey.com/story/news/state/2023/04/03/artificial-intelligence-race-gender-subject-of-njit-2023-women-conference/70056810007/" rel="external nofollow noopener" target="_blank"><strong>read the story</strong></a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Center for Responsible AI at New York University. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. <div class="contact-icons" style="display: flex; justify-content: center; margin: 25px;"> <a href="mailto:%72%65%73%70%6F%6E%73%69%62%6C%65%61%69@%6E%79%75.%65%64%75" title="email"><i class="fas fa-envelope" style="margin-right: 10px; font-size: 52px;"></i></a> <a href="https://airesponsibly.us21.list-manage.com/subscribe?u=fe132ea6ae0280345f9989e41&amp;id=8ebe38f466" title="newsletter" rel="external nofollow noopener" target="_blank"><i class="fab fa-telegram" style="margin-right: 10px; font-size: 52px;"></i></a> <a href="https://www.linkedin.com/company/ai-responsibly" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin" style="margin-right: 10px; font-size: 52px;"></i></a> <a href="https://twitter.com/AIResponsibly" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter" style="margin-right: 10px; font-size: 52px; "></i></a> </div> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>